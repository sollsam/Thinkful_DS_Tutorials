{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts).\n",
    "\n",
    "Note that if you want to use an unprocessed image dataset, scikit-image is a useful package for converting to importable numerics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a dataset containing images os 81 different fruits and implement keras to predict fruit types. The dataset is available in the link below.\n",
    "https://www.kaggle.com/moltean/fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and tests are already available in separate folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the training set\n",
    "train_fruit_img = []\n",
    "train_labels = [] \n",
    "for fruit_path in glob.glob(\"../Omistaja/*/fruits-360/Training/*\"):\n",
    "    fruit_label = fruit_path.split(\"\\\\\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_path, \"*.jpg\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        # Shrink\n",
    "        image = cv2.resize(image, (50, 50))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        train_fruit_img.append(image)\n",
    "        train_labels.append(fruit_label)\n",
    "train_fruit_img = np.array(train_fruit_img)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Apple Braeburn', 'Apple Braeburn', 'Apple Braeburn', ...,\n",
       "       'Walnut', 'Walnut', 'Walnut'], dtype='<U19')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect labels\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Omistaja\\\\fruits\\\\fruits-360\\\\Training\\\\Walnut'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the test set\n",
    "test_fruit_img = []\n",
    "test_labels = [] \n",
    "for fruit_path in glob.glob(\"../Omistaja/*/fruits-360/Test/*\"):\n",
    "    fruit_label = fruit_path.split(\"\\\\\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_path, \"*.jpg\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        image = cv2.resize(image, (50, 50))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        test_fruit_img.append(image)\n",
    "        test_labels.append(fruit_label)\n",
    "test_fruit_img = np.array(test_fruit_img)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels dictionary \n",
    "label_to_id = {v: i for i,v in enumerate(np.unique(train_labels))}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Apple Braeburn',\n",
       " 1: 'Apple Golden 1',\n",
       " 2: 'Apple Golden 2',\n",
       " 3: 'Apple Golden 3',\n",
       " 4: 'Apple Granny Smith',\n",
       " 5: 'Apple Red 1',\n",
       " 6: 'Apple Red 2',\n",
       " 7: 'Apple Red 3',\n",
       " 8: 'Apple Red Delicious',\n",
       " 9: 'Apple Red Yellow',\n",
       " 10: 'Apricot',\n",
       " 11: 'Avocado',\n",
       " 12: 'Avocado ripe',\n",
       " 13: 'Banana',\n",
       " 14: 'Banana Red',\n",
       " 15: 'Cactus fruit',\n",
       " 16: 'Cantaloupe 1',\n",
       " 17: 'Cantaloupe 2',\n",
       " 18: 'Carambula',\n",
       " 19: 'Cherry 1',\n",
       " 20: 'Cherry 2',\n",
       " 21: 'Cherry Rainier',\n",
       " 22: 'Cherry Wax Black',\n",
       " 23: 'Cherry Wax Red',\n",
       " 24: 'Cherry Wax Yellow',\n",
       " 25: 'Clementine',\n",
       " 26: 'Cocos',\n",
       " 27: 'Dates',\n",
       " 28: 'Granadilla',\n",
       " 29: 'Grape Pink',\n",
       " 30: 'Grape White',\n",
       " 31: 'Grape White 2',\n",
       " 32: 'Grapefruit Pink',\n",
       " 33: 'Grapefruit White',\n",
       " 34: 'Guava',\n",
       " 35: 'Huckleberry',\n",
       " 36: 'Kaki',\n",
       " 37: 'Kiwi',\n",
       " 38: 'Kumquats',\n",
       " 39: 'Lemon',\n",
       " 40: 'Lemon Meyer',\n",
       " 41: 'Limes',\n",
       " 42: 'Lychee',\n",
       " 43: 'Mandarine',\n",
       " 44: 'Mango',\n",
       " 45: 'Maracuja',\n",
       " 46: 'Melon Piel de Sapo',\n",
       " 47: 'Mulberry',\n",
       " 48: 'Nectarine',\n",
       " 49: 'Orange',\n",
       " 50: 'Papaya',\n",
       " 51: 'Passion Fruit',\n",
       " 52: 'Peach',\n",
       " 53: 'Peach Flat',\n",
       " 54: 'Pear',\n",
       " 55: 'Pear Abate',\n",
       " 56: 'Pear Monster',\n",
       " 57: 'Pear Williams',\n",
       " 58: 'Pepino',\n",
       " 59: 'Physalis',\n",
       " 60: 'Physalis with Husk',\n",
       " 61: 'Pineapple',\n",
       " 62: 'Pineapple Mini',\n",
       " 63: 'Pitahaya Red',\n",
       " 64: 'Plum',\n",
       " 65: 'Pomegranate',\n",
       " 66: 'Quince',\n",
       " 67: 'Rambutan',\n",
       " 68: 'Raspberry',\n",
       " 69: 'Salak',\n",
       " 70: 'Strawberry',\n",
       " 71: 'Strawberry Wedge',\n",
       " 72: 'Tamarillo',\n",
       " 73: 'Tangelo',\n",
       " 74: 'Tomato 1',\n",
       " 75: 'Tomato 2',\n",
       " 76: 'Tomato 3',\n",
       " 77: 'Tomato 4',\n",
       " 78: 'Tomato Cherry Red',\n",
       " 79: 'Tomato Maroon',\n",
       " 80: 'Walnut'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 81 of fruit types\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 80, 80, 80])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_id = np.array([label_to_id[i] for i in train_labels])\n",
    "test_labels_id = np.array([label_to_id[i] for i in test_labels])\n",
    "test_labels_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "train_fruit_img, test_fruit_img = train_fruit_img / 255.0, test_fruit_img / 255.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x189f363f160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show sample image\n",
    "plt.imshow(train_fruit_img[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning train and test\n",
    "X_train, X_test = train_fruit_img, test_fruit_img\n",
    "y_train, y_test = train_labels_id, test_labels_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float32 for type consistency\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes: (41322, 50, 50, 3) (13877, 50, 50, 3) (41322, 81) (13877, 81)\n",
      "Flattened: (41322, 7500) (13877, 7500)\n"
     ]
    }
   ],
   "source": [
    "#Reshaping to flattened arrays\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], 50*50*3)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], 50*50*3)\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# create 81 binary columns ofr each label\n",
    "y_train = keras.utils.to_categorical(y_train, 81, dtype='float')\n",
    "y_test = keras.utils.to_categorical(y_test, 81, dtype='float')\n",
    "\n",
    "print('Original Sizes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print('Flattened:', X_train_flat.shape, X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron\n",
    "\n",
    "We'll use a sequential model using layers of 128 and 64 nodes respectively with a Rectified Linear Unit activation funcion and a dropout of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               960128    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 81)                5265      \n",
      "=================================================================\n",
      "Total params: 973,649\n",
      "Trainable params: 973,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 41322 samples, validate on 13877 samples\n",
      "Epoch 1/10\n",
      "41322/41322 [==============================] - 25s 611us/step - loss: 3.1882 - acc: 0.1938 - val_loss: 1.6782 - val_acc: 0.5119\n",
      "Epoch 2/10\n",
      "41322/41322 [==============================] - 14s 328us/step - loss: 1.7532 - acc: 0.4698 - val_loss: 1.1047 - val_acc: 0.6639\n",
      "Epoch 3/10\n",
      "41322/41322 [==============================] - 13s 315us/step - loss: 1.3546 - acc: 0.5818 - val_loss: 0.8481 - val_acc: 0.7396\n",
      "Epoch 4/10\n",
      "41322/41322 [==============================] - 16s 396us/step - loss: 1.1465 - acc: 0.6441 - val_loss: 0.8920 - val_acc: 0.6942\n",
      "Epoch 5/10\n",
      "41322/41322 [==============================] - 12s 301us/step - loss: 1.0092 - acc: 0.6827 - val_loss: 0.6589 - val_acc: 0.8241\n",
      "Epoch 6/10\n",
      "41322/41322 [==============================] - 13s 311us/step - loss: 0.9063 - acc: 0.7141 - val_loss: 0.5401 - val_acc: 0.8429\n",
      "Epoch 7/10\n",
      "41322/41322 [==============================] - 14s 338us/step - loss: 0.8426 - acc: 0.7361 - val_loss: 0.7177 - val_acc: 0.7551\n",
      "Epoch 8/10\n",
      "41322/41322 [==============================] - 18s 444us/step - loss: 0.7729 - acc: 0.7585 - val_loss: 0.4667 - val_acc: 0.8523\n",
      "Epoch 9/10\n",
      "41322/41322 [==============================] - 15s 367us/step - loss: 0.7307 - acc: 0.7714 - val_loss: 0.6549 - val_acc: 0.7837\n",
      "Epoch 10/10\n",
      "41322/41322 [==============================] - 13s 309us/step - loss: 0.6838 - acc: 0.7876 - val_loss: 0.5338 - val_acc: 0.8254\n",
      "Test loss: 0.5337951074654697\n",
      "Test accuracy: 0.8253945377242919\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_flat.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(81, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train_flat, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_flat, y_test))\n",
    "score = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the model by adding more dense layers.Each layer will have: 256, 256, 128, and 64, nodes respectively. We'll still use the Rectified Linear Unit activation funcion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               1920256   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 81)                5265      \n",
      "=================================================================\n",
      "Total params: 2,032,465\n",
      "Trainable params: 2,032,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 41322 samples, validate on 13877 samples\n",
      "Epoch 1/10\n",
      "41322/41322 [==============================] - 30s 733us/step - loss: 3.1577 - acc: 0.1804 - val_loss: 2.0634 - val_acc: 0.3868\n",
      "Epoch 2/10\n",
      "41322/41322 [==============================] - 21s 513us/step - loss: 1.5653 - acc: 0.4994 - val_loss: 0.8033 - val_acc: 0.7455\n",
      "Epoch 3/10\n",
      "41322/41322 [==============================] - 20s 493us/step - loss: 1.0481 - acc: 0.6589 - val_loss: 0.6029 - val_acc: 0.8020\n",
      "Epoch 4/10\n",
      "41322/41322 [==============================] - 20s 491us/step - loss: 0.8160 - acc: 0.7341 - val_loss: 0.5994 - val_acc: 0.7984\n",
      "Epoch 5/10\n",
      "41322/41322 [==============================] - 20s 491us/step - loss: 0.6645 - acc: 0.7800 - val_loss: 0.5402 - val_acc: 0.8335\n",
      "Epoch 6/10\n",
      "41322/41322 [==============================] - 21s 503us/step - loss: 0.5764 - acc: 0.8102 - val_loss: 0.5549 - val_acc: 0.8206\n",
      "Epoch 7/10\n",
      "41322/41322 [==============================] - 20s 494us/step - loss: 0.4990 - acc: 0.8380 - val_loss: 0.5385 - val_acc: 0.8234\n",
      "Epoch 8/10\n",
      "41322/41322 [==============================] - 20s 494us/step - loss: 0.4448 - acc: 0.8555 - val_loss: 0.3502 - val_acc: 0.8792\n",
      "Epoch 9/10\n",
      "41322/41322 [==============================] - 20s 492us/step - loss: 0.4131 - acc: 0.8653 - val_loss: 0.3264 - val_acc: 0.9064\n",
      "Epoch 10/10\n",
      "41322/41322 [==============================] - 20s 494us/step - loss: 0.3741 - acc: 0.8786 - val_loss: 0.4025 - val_acc: 0.8726\n",
      "Test loss: 0.40246581971165507\n",
      "Test accuracy: 0.8725949412697269\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(256, activation='relu', input_shape=(X_train_flat.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(81, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train_flat, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_flat, y_test))\n",
    "score = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've managed to increase the accuracy by adding more layers with more nodes. Now let's use Convolutional Neural Networks and try to increse the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a Convolutional Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41322 samples, validate on 13877 samples\n",
      "Epoch 1/10\n",
      "41322/41322 [==============================] - 447s 11ms/step - loss: 3.2145 - acc: 0.1949 - val_loss: 1.0897 - val_acc: 0.6889\n",
      "Epoch 2/10\n",
      "41322/41322 [==============================] - 431s 10ms/step - loss: 1.0717 - acc: 0.6567 - val_loss: 0.3127 - val_acc: 0.8970\n",
      "Epoch 3/10\n",
      "41322/41322 [==============================] - 424s 10ms/step - loss: 0.5167 - acc: 0.8231 - val_loss: 0.2411 - val_acc: 0.9199\n",
      "Epoch 4/10\n",
      "41322/41322 [==============================] - 420s 10ms/step - loss: 0.3511 - acc: 0.8777 - val_loss: 0.1761 - val_acc: 0.9476\n",
      "Epoch 5/10\n",
      "41322/41322 [==============================] - 419s 10ms/step - loss: 0.2714 - acc: 0.9026 - val_loss: 0.1404 - val_acc: 0.9581\n",
      "Epoch 6/10\n",
      "41322/41322 [==============================] - 419s 10ms/step - loss: 0.2175 - acc: 0.9211 - val_loss: 0.1385 - val_acc: 0.9515\n",
      "Epoch 7/10\n",
      "41322/41322 [==============================] - 418s 10ms/step - loss: 0.1873 - acc: 0.9328 - val_loss: 0.1365 - val_acc: 0.9592\n",
      "Epoch 8/10\n",
      "41322/41322 [==============================] - 420s 10ms/step - loss: 0.1666 - acc: 0.9383 - val_loss: 0.1203 - val_acc: 0.9642\n",
      "Epoch 9/10\n",
      "41322/41322 [==============================] - 419s 10ms/step - loss: 0.1506 - acc: 0.9444 - val_loss: 0.1325 - val_acc: 0.9651\n",
      "Epoch 10/10\n",
      "41322/41322 [==============================] - 428s 10ms/step - loss: 0.1356 - acc: 0.9482 - val_loss: 0.1035 - val_acc: 0.9686\n",
      "Test loss: 0.10345630674010495\n",
      "Test accuracy: 0.9685811054262449\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "num_classes = 81\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(50, 50, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Multi Layer Perceptron models run faster and return a reasonably good accuracy. For a higher accuracy Convolutional Neural Network model is a clear winner but it comes with higher runtime.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
