{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Classifier from previous assignment \n",
    "\n",
    "amazon = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)#, delimiter= '\\t', header=None)\n",
    "amazon.columns = ['review', 'score']\n",
    "\n",
    "keywords = ['bad', 'littered', 'awful', 'but', 'poor', 'no', 'borring', 'cheap', 'dislike'\n",
    "           , 'empty', 'hollow', 'waste', 'worst', \"don't\", \"can't\", 'hate', 'flaws', 'pathetic', 'atrocity', 'maybe']\n",
    "for key in keywords:\n",
    "    amazon[str(key)] = amazon.review.str.contains(' ' + str(key) + ' ', case=False)\n",
    "    \n",
    "amazon['allcaps'] = amazon.review.str.isupper()\n",
    "\n",
    "sns.heatmap(amazon.corr(), cmap=\"YlGnBu\")\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "data_A = amazon[keywords + ['allcaps']]\n",
    "target_A = amazon['score']\n",
    "bnb.fit(data_A, target_A)\n",
    "y_pred_amazon = bnb.predict(data_A)\n",
    "\n",
    "print('For Amazon reviews, Number of mislabelled points out of a total {} points : {}'.format(data_A.shape[0], (target_A != y_pred_amazon).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target_A, y_pred_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =', (1000-450)*100/1000,'%')\n",
    "print('Total:', data_A.shape[0])\n",
    "print('True Negative = 88')\n",
    "print('True Positive = 462')\n",
    "print('False Negative = 38')\n",
    "print('False Positive = 412')\n",
    "print('Sensitivity=', 462 *100/(462+38),'%')\n",
    "print('Specificity =', 88 * 100 /(88+412),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating five different versions of the classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Iteration 1. Tune keywords and use pattern matching</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_1 = amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_1 = ['bad', 'awful', 'but', 'poor', 'no', 'borring', 'cheap', 'empty', 'hollow', 'waste', 'worst', \"can't\", 'hate', 'flaws', 'pathetic', 'atrocity', 'maybe']\n",
    "\n",
    "for key in keywords_1:\n",
    "    amazon_1[str(key)] = amazon_1.review.str.contains(str(key), case=False)\n",
    "    \n",
    "amazon_1['allcaps'] = amazon_1.review.str.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(amazon_1.corr(), cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = amazon_1[keywords + ['allcaps']]\n",
    "target_1 = amazon_1['score']\n",
    "bnb.fit(data_1, target_1)\n",
    "y_pred_amazon_1 = bnb.predict(data_1)\n",
    "\n",
    "print('For Amazon reviews, Number of mislabelled points out of a total {} points : {}'.format(data_1.shape[0], (target_1 != y_pred_amazon_1).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target_1, y_pred_amazon_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =', (1000-337)*100/1000,'%')\n",
    "print('Total:', data_1.shape[0])\n",
    "print('True Negative = 223')\n",
    "print('True Positive = 440')\n",
    "print('False Negative = 60')\n",
    "print('False Positive = 277')\n",
    "print('Sensitivity=', 440 *100/(440+60),'%')\n",
    "print('Specificity =', 223 * 100 /(223+277),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Iteration 2. More keywords and tuning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_2 = amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_2 = ['bad', 'waste', 'but', 'poor', 'none', 'short', 'cheap', 'ever', 'mistake', 'hype', 'worst', 'cannot', 'hate', 'flaws', 'useless', 'lost',\n",
    "              'only', 'money', 'trash']\n",
    "\n",
    "for key in keywords_2:\n",
    "    amazon_2[str(key)] = amazon_2.review.str.contains(' '+ str(key)+ ' ', case=False)\n",
    "    \n",
    "amazon_2['allcaps'] = amazon_2.review.str.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(amazon_1.corr(), cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = amazon_2[keywords + ['allcaps']]\n",
    "target_2 = amazon_2['score']\n",
    "bnb.fit(data_2, target_2)\n",
    "y_pred_amazon_2 = bnb.predict(data_2)\n",
    "\n",
    "print('For Amazon reviews, Number of mislabelled points out of a total {} points : {}'.format(data_2.shape[0], (target_2 != y_pred_amazon_2).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(target_2, y_pred_amazon_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =', (1000-370)*100/1000,'%')\n",
    "print('Total:', data_2.shape[0])\n",
    "print('True Negative = 189')\n",
    "print('True Positive = 441')\n",
    "print('False Negative = 59')\n",
    "print('False Positive = 311')\n",
    "print('Sensitivity=', 441 *100/(441+59),'%')\n",
    "print('Specificity =', 189 * 100 /(189+311),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Iteration 3. Remove a feature</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the all caps feature and use the original array of keywords\n",
    "amazon_3 = amazon \n",
    "data_3 = amazon_3[keywords]\n",
    "target_3 = amazon_3['score']\n",
    "bnb.fit(data_3, target_3)\n",
    "y_pred_amazon_3 = bnb.predict(data_3)\n",
    "\n",
    "print('For Amazon reviews, Number of mislabelled points out of a total {} points : {}'.format(data_3.shape[0], (target_3 != y_pred_amazon_3).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(target_3, y_pred_amazon_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =', (1000-371)*100/1000,'%')\n",
    "print('Total:', data_3.shape[0])\n",
    "print('True Negative = 183')\n",
    "print('True Positive = 446')\n",
    "print('False Negative = 54')\n",
    "print('False Positive = 317')\n",
    "print('Sensitivity=', 446 *100/(446+54),'%')\n",
    "print('Specificity =', 183 * 100 /(183+317),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables are independent a shown in the heatmap and hence the classifires won't overfit. \n",
    "\n",
    "The first itteration of classifiers seem to perform better with a relatively high accuracy, sensitivity, and specificity. This is due to pattern matching in the keyworks used instead of word match. Over all the keywords feature is most impactful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
